0
En los equipos de laboratorio se dispone de un solo procesador con 4 nucleos con una frecuencia máxima de 3,4GHz y sin hyperthreading, puesto que hay el mismo número de threads (siblings) que de cores físicos.

1.1
sí, se pueden lanzar más threads que cores tenga un sistema. Pero esto no siempre tiene sentido pues no todos los procesadores soportan multithreading y por tanto, en los casos en los que no se soporte esto, no habría un aumento de rendimiento l lanzar más threads que cores, sin embargo si un sistema si soporta esta tcnología sí que tendría sentido lanzar hasta 2 veces más threads que cores.

1.2
Como los ordenadores del laboratorio disponen de 4 cores sin hyperthreading, solo tendiría sentido usar hasta 4 threads (tantos como cores), en el caso del cluster tendría sentido ejecutar hasta 128 threads pues este dispone de 16 procesadores cada uno de los cuales tiene 8 cores, lo que suma un total de 128 cores que, ya que no sosportan hyperthreading solo tiene sentido que ejcuten un hilo cada uno, lo que supone que más de 128 threads a la vz no tienen sentido.
En nuestro ordenador personal tendría sentido ejecutar hasta 12 threads a la vez pues dispone de un único procesador con 6 cores, pero estos sí que disponen de hyperthreading pudiendo ejecutar de forma eficinte hasta 2 hilos cada core, lo que hace que tenga sentido ejecutar hasta 12 threads a la vez.

1.3
Cuano declaramos una variable privada openMP crea una variable independiente con el mismo nombre para cada thread. Si uno de los threads modifica su variable, la variable del mismo nombre del resto de threads no se modifica pues son variables independientes.

1.4
El valor de las variables privadas se inicializa a 0.

1.5
Cuando finaliza la región paralela, la variable mantiene el valor que tenía antes de la región paralela.

1.6
En el caso de las variables públicas, estas son compartidas por todos los threads, y los cambios que realice en ella un thread se ven reflejados en el resto.
Al finalizar la región paralela, esta variable mantiene el valor que tenía al final de la región paralela (tras ser modificada por el último thread).

2.1/2.2
El resultado es correcto en caso pescalar_serie pues no emplea threads por lo que no puede habr problemas de sincronización, y tambien es correcto en el caso pescalar_par2 pues al incluir en la declaración de la región de openMP el atributo "reduction(+:sum)" openMP crea una copia interna dentro de cada thread y al acabar la región paralela suma estas copias internas evitando así problemas de sincronización. Esto no se realiza en el caso pescalar_par1 haciendo que surjan en este problemas de sincronización lo que provoca que en este caso el resultado sea erróneo. 


3.1 La versión del programa la que obtiene el peor rendimiento es la que paraleliza el bucle 1, es decir, el bucle mas externo de todos.

3.2 La versión del programa con mejor rendimiento es la qu eparaleliza el bucle 2, es decir el bucle intermedio.

3.3 En las gráficas se puede observar en primero lugar que el tiempo de ejecución es mucho menor en la version en paralelo que en serie, como era de esperar, ya que la versión en paralelo divide el "trabajo" entre los distintos hilos que se ejecutan, y, tambien como era de esperar, en ambos casos el tiempo de ejecución es mayor cuanto mayor es el tamaño de la matriz, puesto que cuanto más grande sea la matriz mayor será el número de iteraciones que se deberán realizar para llevar a cabo la multiplicación. 
En el caso de la gráfica de la aceleración se observa claramente como esta disminuye cuando aumenta el tamaño de la matriz, esto se debe a que cuanto mayor sea el tamaño de la matriz mayor será la eficiencia del programa que realiza el proceso en paralelo (es decir, tardará menos en comparacion con el de serie) mientras que mayor será el tiempo que tarda el programa en serie, y, como la aceleración se calcula como tiempoParalelo/tiempoSerie como el tiempo en serie aumenta más que el tiempo en paralelo, estra aceleracion se aproxima a cero a medida que aumenta el tamaño. Cabe destacar que cuando las matrices comienzan a ser muy grandes, la aceleración comienza a estabilizarse en valores muy próximos a cero, esto se debe a que cuando el tiempo en paralelo es muy pequeño en comparación con el de serie, la aceleración se queda muy próxima a cero y por mucho que el tiempo en serie continúe aumentando la aceleración disminuira muy poco notablemente y que estará muy cerca de su minimo que sería cero (nunca llega a él).

4.1 100.000.000

4.2 La diferencia principal entre estos 2 programas es que el pi_par1 va actualizando constantemente el valor que ha calculado en el array compartido mientras que pi_par4 calcula primero el valor final y despues lo guarda en la posición correspondiente del array compartido, por lo tanto, pi_par1 va accediendo constantemente a este array compartido mientras que pi_par4 solamente necesita hacer un acceso a este al finaalizar el cálculo para guardar el dato.

4.3 En ambas versiones el resultado que se calcula es exactamente igual, sin embargo pi_par4 lo calcula en mucho menor tiempo que pi_par1, aproximadamente 10 veces mas rapido en el ordenado en que lo hemos probado. Esto probablemente se debe a que, como hemos comentado en el apartado anterior, en pi_par1 cada uno de los hilos ejecutados accede al array compartido y modifica el campo que le corresponde cada vez que calcula el valor del area de uno de los rectangulos, accediendo así un gran número de veces a este, sin embargo en pi_par4 cada hilo accede tan solo 1 vez al terminar de calcular la suma de las áreas de todos los rectángulos que le corresponden en una variable propia. De este modo, como en pi_par4 se tiene que acceder mucho menos al array compartido se da menor competencia, y los hilos tienen menor probabilidad de tener que esperar a que otro termine de acceder al array compartido para poder acceder él a modificarlo, por lo que tarda menos.

4.4 Al ejecitar las versiones pi_par2 y pi_par3 vemos que, aunque ambas dan el resultado correcto, pi_par2 no obtiene la mejora esperada de tiempo, de hecho practicamente no se aprecia diferencia en el tiempo que tarda con respecto a pi_par1, sin embargo pi_par3 si tarda el tiempo esperado aproximandose mucho al tiempo de pi_par4.

4.5 Al modificar la linea 32 de pi_par3 para que tome los valores 1, 2, 4, 6, 7, 8, 9, 10 y 12 se observa con bastante claridad que cuanto mayor es este valor, el tiempo que tarda el programa en calcular el valor de pi es menor, por lo tanto el rendimiento mejora cuanto mayor es este campo.

5.1 al usar la directiva critical lo que se consigue es que ese fragmento de código solo lo pueda ejecutar 1 hilo a la vez de modo que no haya varios hilos que modifiquen a la vez el valor pi pues si lo hiciesen la suma acabaría siendo errónea, sin embargo esto hace que se tengan que esperar unos hilos a otros haciendo que la ejecución tarde más, como ocurre en pi_par5, mientras que en pi_par4 tarda mucho menos pues los hilos no necesitan ir esperando a entrar en ninguna seccion del código. Sin embargo lo que mas destaca al ejecutar pi_par5 es que esta versión da resultados incorrectos y diferentes cada vez, esto se debe a que el parametro i que utiliza cada uno de los hilos para calcular su parte de pi no es privado, y por tanto cuando uno lo modifica, se modifica en todos los hilos, haciendo que haya iteraciones de los bucles de los distintos hilos que no se ejecuten y por tanto acabando en un resultado erróneo.

5.2 Entre pi_par6 y pi_par7 existe una gran diferencia de rendimiento similar a la diferencia explicada entre pi_par1 y pi_par4. En este caso pi_par6 es el que tarda un tiempo superior mientras que pi_par7 tarda mucho menos en realizar el cálculo de pi. Esto se debe aa que en pi_par6 cada uno de los hilos debe ir modificando un array compartido casi constantemente (como en pi_par1) por lo que se produce un cuello de botella ya que tan solo 1 hilo puede acceder al array a la vez, mientras que pi_par7 no emplea este array compartido de modo que no tiene este cuello de botella. En estas versiones además observamos 2 directivas que no habíamos visto en las anteriores. En primero lugar en pi_par6 vemos "omp for" que se emplea para que el bucle for que aparece justo despues se ejecute con contadores "i" independientes a pesar de que la i no está declarada como variable privada. La otra directiva aparece en pi_par7 y es "omp parallel for reduction(+: sum)" Esta directiva sirve para que el bucle for al que se refiere se realize de forma paralela por varios hilos y para que cada uno emplee una variable privada "sum" para ello y, tras terminar la ejecución todos los hilos, se unan todas estas variables "sum" privadas en una sola con la operación especificada en el reduction, que, en este caso es la suma; por lo tanto tras la ejecución de todos los hilos se tendría una única variable "sum" cuyo valor es el de la suma de todas las variables "sum" privadas de cada uno de los hilos.


